{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1_t4V9bnt-hB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36d84e010817fee9ca731581dcbc354a",
     "grade": false,
     "grade_id": "cell-e911fa75d4ae6ea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Hyperparameter Optimization For The Human Freedom Index Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "The <a href=\"https://www.cato.org/human-freedom-index/2021 \">Human Freedom Index</a> measures economic freedoms such as the freedom to trade or to use sound money, and it captures the degree to which people are free to enjoy the major freedoms often referred to as civil liberties—freedom of speech, religion, association, and assembly— in the countries in the survey. In addition, it includes indicators on rule of law, crime and violence, freedom of movement, and legal discrimination against same-sex relationships. We also include nine variables pertaining to women-specific freedoms that are found in various categories of the index.\n",
    "\n",
    "Datasource: https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\n",
    "\n",
    "<u>Citation</u>\n",
    "\n",
    "Ian Vásquez, Fred McMahon, Ryan Murphy, and Guillermina Sutter Schneider, The Human Freedom Index 2021: A Global Measurement of Personal, Civil, and Economic Freedom (Washington: Cato Institute and the Fraser Institute, 2021).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0f1f0289d715c8a9dce469c89a4174b",
     "grade": false,
     "grade_id": "cell-0d9a7d4e70c072c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JdETeU66gS1E",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbd36cda059448e32e30917ff68670ee",
     "grade": false,
     "grade_id": "cell-8f995c9cdf882820",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "I loaded the Human Freedom Index data from the provided link into a DataFrame called df, dropped redundant columns, and stored the independent variables in a DataFrame called X and the dependent variable hf_quartile in a DataFrame called y.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3416f03882927817c5373de161fb4a59",
     "grade": false,
     "grade_id": "ex1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Create df\n",
    "def read_df(link):\n",
    "    df = pd.read_csv(link)\n",
    "    return df\n",
    "    \n",
    "df = read_df(\"https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b08e1fb0583f319d0f244af07c42f2b5",
     "grade": true,
     "grade_id": "test1_1",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Drop redundant columns\n",
    "cols_to_drop = ['year','ISO','countries']\n",
    "words_drop = ['rank','score']\n",
    "\n",
    "def drop_columns(data,cols,words_drop):\n",
    "    #Drop specified columns\n",
    "    data = data.drop(cols,axis = 1)\n",
    "    #Drop columns with the word \"rank\" and \"score\"\n",
    "    for i in range(len(words_drop)):\n",
    "        data = data[data.columns.drop(list(data.filter(regex=words_drop[i])))]\n",
    "        \n",
    "    return (data)\n",
    "\n",
    "#Drop NA values in target variable\n",
    "target = 'hf_quartile'\n",
    "def drop_na_target(data,target_var):\n",
    "    data = data.dropna(subset=[target_var])\n",
    "    return data\n",
    "\n",
    "#Assign X and y:\n",
    "def assign_x_y(data,target_var):\n",
    "    X = data.drop(columns=target_var)\n",
    "    y = data[target_var]\n",
    "    return (X,y)\n",
    "\n",
    "\n",
    "df = drop_columns(df,cols_to_drop,words_drop)\n",
    "df = drop_na_target(df,target)\n",
    "(X,y) = assign_x_y(df,target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I4Ai5L-bgQI-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bc13605b63554dd684e5c3359109498",
     "grade": false,
     "grade_id": "cell-cac60b3d5a84bc9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "I created a Pipeline with a SimpleImputer using the most frequent strategy, a OneHotEncoder for categorical variables, a standard scaler, and a logistic regression model with the solver saga and max_iter 2000. The resulting pipeline is stored in a variable called pipe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e312a6da82f8c4bc37df98d9182e9662",
     "grade": false,
     "grade_id": "ex2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Define categorical and numerical variables\n",
    "def cat_num_var(independent_var):\n",
    "    \"\"\"Function that creates a list with numerical variables and a list\n",
    "    with categorical variables\"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    for col in independent_var.columns:\n",
    "        if independent_var[col].dtype == object:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            numerical_features.append(col)\n",
    "    return (categorical_features,numerical_features)\n",
    "\n",
    "#Transformer\n",
    "def transform_col(i=int): #i = position of column to transform\n",
    "    \"\"\"Function that creates a transformer for the pipeline\"\"\"\n",
    "    transformer = ColumnTransformer([(\"ohe_encoder\", OneHotEncoder(sparse = False), [i])],\n",
    "                                                                remainder = \"passthrough\")\n",
    "    return transformer\n",
    "\n",
    "#steps for pipeline\n",
    "def steps(transformer):\n",
    "    steps = [(\"mode\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "            (\"data_cleaning\", transformer),            \n",
    "            (\"normalization\", StandardScaler()),\n",
    "            (\"training\", LogisticRegression(solver='saga',max_iter=2000))]\n",
    "    return steps\n",
    "\n",
    "(categorical_features,numerical_features) = cat_num_var(X)\n",
    "transformer = transform_col(0)\n",
    "steps = steps(transformer)\n",
    "pipe = Pipeline(steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I4Ai5L-bgQI-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd5fab28e05b69b65eb89f8d17a57943",
     "grade": false,
     "grade_id": "cell-cac60b3d5a84bcasdfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Cross-validation with three stratified folds to estimate the performance of the model and stored the test score values in a dictionary called fold_scores.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "id": "2EZ4PgrhYpcn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b315830f2239f6d81c0964227610ff",
     "grade": false,
     "grade_id": "ex3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9181380417335474, 2: 0.9501607717041801, 3: 0.8954983922829582}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Train-test split\n",
    "def traintest_split(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "#Cross Validation\n",
    "def cross_validations(pipe, X, y, n_folds = int):\n",
    "    scores = cross_val_score(pipe, X, y, cv = n_folds)\n",
    "    fold_scores = {}\n",
    "    for k in range(1, len(scores)+1):\n",
    "        fold_scores[k] = scores[k-1]\n",
    "    return fold_scores\n",
    "\n",
    "\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = traintest_split(X,y)\n",
    "fold_scores = cross_validations(pipe,X, y,3)\n",
    "fold_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I4Ai5L-bgQI-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfdf6a55bc7ea4dc37bddbc8066c40ff",
     "grade": false,
     "grade_id": "cell-cac60b3d5artw84bcasdfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">    \n",
    "I created a GridSearchCV object called grid with three folds, using the previous pipeline. The grid search object tests the hyperparameters penalty with values ['l1', 'l2'] and C with values [0.1, 10]. I fitted the grid search object using the train and test datasets correctly. The best achieved accuracy score is stored in a variable called score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00ef2091bcd0c6ccc4ed66e901f1fff9",
     "grade": false,
     "grade_id": "ex4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9491978609625669"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'training__penalty':['l1', 'l2'], 'training__C':[0.1,10]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 3)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "grid.best_score_\n",
    "score = grid.best_estimator_.score(X_test,y_test)\n",
    "score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I4Ai5L-bgQI-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f18076388ac95878aba1a2591b3953d",
     "grade": false,
     "grade_id": "cell-cac60b3d5artw84ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">    \n",
    "To optimize hyperparameters for all the steps of the pipeline, I created a new GridSearchCV object called grid and included hyperparameters from the scaler, imputer, transformer, encoder, and model. This open-ended approach allows for testing a wide range of hyperparameters for each step, ensuring comprehensive optimization.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variables that represent a Model \n",
    "LR= LogisticRegression()\n",
    "DT= DecisionTreeClassifier()\n",
    "RF= RandomForestClassifier()\n",
    "\n",
    "#Creating the transformer to make the pipelines\n",
    "onehotencoder= OneHotEncoder(sparse=False)\n",
    "imputer= SimpleImputer(strategy='most_frequent')\n",
    "inner_pipe_steps = [('impute', imputer), ('ohe', onehotencoder)]\n",
    "inner_pipe= Pipeline(inner_pipe_steps)\n",
    "transformer = ColumnTransformer([('inner', inner_pipe, categorical_features)], remainder = 'passthrough')\n",
    "\n",
    "#Creating the pipeline for Logistic Regression\n",
    "pipe_steps=[('preprocess', transformer),\n",
    "            ('imputer', SimpleImputer()),\n",
    "            ('scaler', StandardScaler()), \n",
    "            ('classifier', DT)]\n",
    "pipe= Pipeline(pipe_steps)\n",
    "\n",
    "#Define the parameter grid for the LR\n",
    "param_gridlr = {\n",
    "    'classifier':[LR],\n",
    "    'imputer__strategy' : ['median', 'mean'],\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'scaler__with_std': [True, False],\n",
    "    'classifier__C': [0.1, 1.0, 10.0], \n",
    "    'classifier__penalty' :  ['l1', 'l2']\n",
    "}\n",
    "\n",
    "\n",
    "#Define the parameter grid for the DT\n",
    "param_griddt = {\n",
    "    'classifier': [DT],\n",
    "    'imputer__strategy' : ['median', 'mean'],\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'scaler__with_std': [True, False],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, None],\n",
    "    'classifier__min_samples_split': [2, 3, 4, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "#Define the parameter grid for the RF\n",
    "param_gridrf = {\n",
    "    'classifier': [RF],\n",
    "    'imputer__strategy': ['median' 'mean'],\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'scaler__with_std': [True, False],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, None],\n",
    "    'classifier__min_samples_split': [2, 3, 4, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "#Perform the grid search for Decision Tree\n",
    "grid_param_list = [param_gridlr, param_griddt, param_gridrf]\n",
    "grid_search = GridSearchCV(pipe, grid_param_list, cv=3, n_jobs=-2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Best_parameters= grid_search.best_params_\n",
    "Best_score = grid_search.best_score_\n",
    "Best_estimator = grid_search.best_estimator_.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LogisticRegression(), 'classifier__C': 1.0, 'classifier__penalty': 'l2', 'imputer__strategy': 'mean', 'scaler__with_mean': True, 'scaler__with_std': True}\n",
      "0.9397078589340596\n",
      "0.9518716577540107\n"
     ]
    }
   ],
   "source": [
    "print(Best_parameters)\n",
    "print(Best_score)\n",
    "print(Best_estimator)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session I.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "345717bfb8187cb00a0ab02a6be2898e49f38e8c4e09e413faabc6f6c3adc1b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
